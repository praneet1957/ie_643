{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "def_train_eval.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1W7JmWfeFIT-tW9ngkT0TyfqQBIh6My3v",
      "authorship_tag": "ABX9TyPb1xpqm324b/xTpbvwD3MI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/praneet1957/ie_643/blob/main/def_train_eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "lVvpsZ4sS6Zw",
        "outputId": "c0633ded-97fa-4d61-e0d4-e3842b55aed8"
      },
      "source": [
        "import sys\r\n",
        "import os\r\n",
        "sys.path.append('..')\r\n",
        "import time\r\n",
        "import torch.utils.data as utils\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torch import optim\r\n",
        "import torch.nn.functional as F\r\n",
        "import numpy as np\r\n",
        "from /content/drive/MyDrive/Colab Notebooks/models.ipynb import *   \r\n",
        "from sklearn.cluster import SpectralClustering , KMeans\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from scipy.sparse.linalg import eigs\r\n",
        "from torch.autograd import Variable\r\n",
        "\r\n",
        "device = torch.device(\"cuda\")\r\n",
        "BATCH_SIZE=128\r\n",
        "MU = 5\r\n",
        "MODEL_LOC = 'resources/trained_models/Ours/{}'\r\n",
        "\r\n",
        "\r\n",
        "def load_batch(index, size, seq_ID, train_sequence_stream1, pred_sequence_stream_1, train_sequence_stream2, pred_sequence_stream2, train_eig_seq, pred_eig_seq):\r\n",
        "    '''\r\n",
        "    to load a batch of data\r\n",
        "    :param index: index of the batch\r\n",
        "    :param size: size of the batch of data\r\n",
        "    :param seq_ID: either train sequence or a pred sequence, give as a str\r\n",
        "    :param train_sequence: list of dicts of train sequences\r\n",
        "    :param pred_sequence: list of dicts of pred sequences\r\n",
        "    :return: Batch mof data\r\n",
        "    '''\r\n",
        "\r\n",
        "    i = index\r\n",
        "    batch_size = size\r\n",
        "    start_index = i * batch_size\r\n",
        "    stop_index = (i+1) * batch_size\r\n",
        "\r\n",
        "    if stop_index >= len(train_sequence_stream1):\r\n",
        "        stop_index = len(train_sequence_stream1)\r\n",
        "        start_index = stop_index - batch_size\r\n",
        "    if seq_ID == 'train':\r\n",
        "        stream1_train_batch = train_sequence_stream1[start_index:stop_index]\r\n",
        "        stream2_train_batch = train_sequence_stream2[start_index:stop_index]\r\n",
        "        eigs = train_eig_seq[start_index:stop_index]\r\n",
        "        single_batch = [stream1_train_batch, stream2_train_batch, eigs]\r\n",
        "\r\n",
        "    elif seq_ID == 'pred':\r\n",
        "        stream1_pred_batch = pred_sequence_stream_1[start_index:stop_index]\r\n",
        "        stream2_pred_batch = pred_sequence_stream2[start_index:stop_index]\r\n",
        "        eigs = pred_eig_seq[start_index:stop_index]\r\n",
        "        single_batch = [stream1_pred_batch, stream2_pred_batch, eigs]\r\n",
        "    else:\r\n",
        "        single_batch = None\r\n",
        "        print('please enter the sequence ID. enter train for train sequence or pred for pred sequence')\r\n",
        "    return single_batch\r\n",
        "\r\n",
        "\r\n",
        "def trainIters(n_epochs, train_dataloader, valid_dataloader, train2_dataloader,valid2_dataloader, train_eig, val_eig, data, sufix, s2, print_every=1, plot_every=1000, learning_rate=1e-3, save_every=5):\r\n",
        "    start = time.time()\r\n",
        "    plot_losses_stream1 = []\r\n",
        "    plot_losses_stream2 = []\r\n",
        "\r\n",
        "    output_stream2_decoder = None\r\n",
        "    num_batches = int(len(train_dataloader)/BATCH_SIZE)\r\n",
        "    # Stream1 Data\r\n",
        "    # inputs , labels = next ( iter ( train_dataloader ) )\r\n",
        "    # [ batch_size , step_size , fea_size ] = inputs.size ()\r\n",
        "    # input_dim = fea_size\r\n",
        "    # hidden_dim = fea_size\r\n",
        "    # output_dim = fea_size\r\n",
        "    encoder_stream1 = None \r\n",
        "    decoder_stream1 = None\r\n",
        "    encoder_stream2 = None\r\n",
        "    decoder_stream2 = None\r\n",
        "    encoder1loc = os.path.join(MODEL_LOC.format(data), 'encoder_stream1_{}{}.pt'.format(data, sufix))\r\n",
        "    decoder1loc = os.path.join(MODEL_LOC.format(data), 'decoder_stream1_{}{}.pt'.format(data, sufix)) \r\n",
        "    \r\n",
        "    train_raw = train_dataloader\r\n",
        "    pred_raw = valid_dataloader\r\n",
        "    train2_raw = train2_dataloader\r\n",
        "    pred2_raw = valid2_dataloader\r\n",
        "    train_eig_raw = train_eig\r\n",
        "    pred_eig_raw = val_eig\r\n",
        "    # Initialize encoder, decoders for both streams\r\n",
        "    batch = load_batch ( 0 , BATCH_SIZE , 'pred' , train_raw , pred_raw , train2_raw , pred2_raw, train_eig_raw, pred_eig_raw)\r\n",
        "    batch , _, _= batch\r\n",
        "    batch_in_form = np.asarray ( [ batch[ i ][ 'sequence' ] for i in range ( BATCH_SIZE ) ] )\r\n",
        "    batch_in_form = torch.Tensor ( batch_in_form )\r\n",
        "    [ batch_size , step_size , fea_size ] = np.shape(batch_in_form)\r\n",
        "    input_dim = fea_size\r\n",
        "    hidden_dim = fea_size\r\n",
        "    output_dim = fea_size\r\n",
        "\r\n",
        "    encoder_stream1 = Encoder ( input_dim , hidden_dim , output_dim ).to ( device )\r\n",
        "    decoder_stream1 = Decoder ( 's1' , input_dim , hidden_dim , output_dim, batch_size, step_size ).to ( device )\r\n",
        "    encoder_stream1_optimizer = optim.RMSprop(encoder_stream1.parameters(), lr=learning_rate)\r\n",
        "    decoder_stream1_optimizer = optim.RMSprop(decoder_stream1.parameters(), lr=learning_rate)\r\n",
        "    print(\"loading {}...\".format(encoder1loc))\r\n",
        "    #encoder_stream1.load_state_dict(torch.load(encoder1loc))\r\n",
        "    #encoder_stream1.eval()       \r\n",
        "    #decoder_stream1.load_state_dict(torch.load(decoder1loc))        \r\n",
        "    #decoder_stream1.eval()\r\n",
        "    if s2 is True:\r\n",
        "        batch = load_batch ( 0 , BATCH_SIZE , 'pred' , train_raw , pred_raw , train2_raw , pred2_raw, train_eig_raw, pred_eig_raw )\r\n",
        "        _ , _, batch = batch\r\n",
        "        batch = np.asarray([batch[i] for i in range(len(batch))])\r\n",
        "        batch_in_form = torch.Tensor(batch)\r\n",
        "        [ batch_size , step_size , fea_size ] = batch_in_form.size()\r\n",
        "        input_dim = fea_size\r\n",
        "        hidden_dim = fea_size\r\n",
        "        output_dim = fea_size\r\n",
        "\r\n",
        "        encoder_stream2 = Encoder ( input_dim , hidden_dim , output_dim ).to ( device )\r\n",
        "        decoder_stream2 = Decoder ( 's2', input_dim , hidden_dim , output_dim, batch_size, step_size ).to ( device )\r\n",
        "        encoder_stream2_optimizer = optim.RMSprop(encoder_stream2.parameters(), lr=learning_rate)\r\n",
        "        decoder_stream2_optimizer = optim.RMSprop(decoder_stream2.parameters(), lr=learning_rate)\r\n",
        "\r\n",
        "\r\n",
        "    for epoch in range(0, n_epochs):\r\n",
        "#        print(\"epoch: \", epoch)\r\n",
        "        print_loss_total_stream1 = 0  # Reset every print_every\r\n",
        "        print_loss_total_stream2 = 0  # Reset every plot_every\r\n",
        "        # Prepare train and test batch\r\n",
        "        for bch in range(num_batches):\r\n",
        "            print('# {}/{} epoch {}/{} batch'.format(epoch, n_epochs, bch, num_batches))\r\n",
        "            trainbatch_both = load_batch ( bch , BATCH_SIZE , 'train' , train_raw , pred_raw, train2_raw, pred2_raw, train_eig_raw, pred_eig_raw )\r\n",
        "            trainbatch, train_middle, trainbatch2 = trainbatch_both\r\n",
        "            trainbatch_in_form = np.asarray([trainbatch[i]['sequence'] for i in range(BATCH_SIZE)])\r\n",
        "            trainbatch_in_form = torch.Tensor( trainbatch_in_form ).to(device)\r\n",
        "\r\n",
        "            testbatch_both = load_batch ( bch , BATCH_SIZE , 'pred' , train_raw , pred_raw, train2_raw, pred2_raw, train_eig_raw, pred_eig_raw )\r\n",
        "            testbatch, test_middle, testbatch2 = testbatch_both\r\n",
        "            testbatch_in_form = np.asarray([testbatch[i]['sequence'] for i in range(BATCH_SIZE)])\r\n",
        "            testbatch_in_form =  torch.Tensor(testbatch_in_form ).to(device)\r\n",
        "            # for data in train_dataloader:\r\n",
        "\r\n",
        "            input_stream1_tensor = trainbatch_in_form\r\n",
        "            batch_agent_ids = [trainbatch[i]['agent_ID'] for i in range(BATCH_SIZE)]\r\n",
        "            target_stream1_tensor = testbatch_in_form\r\n",
        "            if s2 is True:\r\n",
        "                trainbatch2 = torch.Tensor(np.asarray([trainbatch2[i] for i in range(len(trainbatch2))]))\r\n",
        "                input_stream2_tensor = trainbatch2\r\n",
        "                input_stream2_tensor = Variable(input_stream2_tensor.to(device))\r\n",
        "                testbatch2 =  torch.Tensor(np.asarray([testbatch2[i] for i in range(len(testbatch2))]))\r\n",
        "                target_stream2_tensor = testbatch2\r\n",
        "                target_stream2_tensor = Variable(target_stream2_tensor.to(device))\r\n",
        "            if s2 is True:\r\n",
        "                loss_stream2, output_stream2_decoder = train_stream2(input_stream2_tensor, target_stream2_tensor, encoder_stream2, decoder_stream2, encoder_stream2_optimizer, decoder_stream2_optimizer)\r\n",
        "                print_loss_total_stream2 += loss_stream2\r\n",
        "\r\n",
        "            loss_stream1 = train_stream1(input_stream1_tensor, target_stream1_tensor, encoder_stream1, decoder_stream1, encoder_stream1_optimizer, decoder_stream1_optimizer, output_stream2_decoder, batch_agent_ids, test_middle, s2)\r\n",
        "            print(loss_stream1)\r\n",
        "            print_loss_total_stream1 += loss_stream1\r\n",
        "                # print(loss_stream1)\r\n",
        "\r\n",
        "            # print_loss_avg_stream1 = print_loss_total_stream1 / print_every\r\n",
        "            # print_loss_total_stream1 = 0\r\n",
        "        print( 'stream1 average loss:', print_loss_total_stream1/num_batches)\r\n",
        "            # print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),epoch, epoch / n_epochs * 100, print_loss_avg_stream1))\r\n",
        "        if s2 is True:\r\n",
        "            # print_loss_avg_stream2 = print_loss_total_stream2 / print_every\r\n",
        "            # print_loss_total_stream2 = 0\r\n",
        "            print( 'stream2 average loss:', print_loss_total_stream2/num_batches)\r\n",
        "\r\n",
        "            # print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),epoch, epoch / n_epochs * 100, print_loss_avg_stream2))\r\n",
        "\r\n",
        "            # if epoch % plot_every == 0:\r\n",
        "            #     if s1 is True:\r\n",
        "            #         plot_loss_avg_stream1 = plot_loss_total_stream1 / plot_every\r\n",
        "            #         plot_losses_stream1.append(plot_loss_avg_stream1)\r\n",
        "            #         plot_loss_total_stream1 = 0\r\n",
        "            #     if s2 is True:\r\n",
        "            #         plot_loss_avg_stream2 = plot_loss_total_stream2 / plot_every\r\n",
        "            #         plot_losses_stream2.append(plot_loss_avg_stream2)\r\n",
        "            #         plot_loss_total_stream2 = 0\r\n",
        "        if epoch % save_every == 0:\r\n",
        "            save_model(encoder_stream1, decoder_stream1, encoder_stream2, decoder_stream2, data, sufix, s2)\r\n",
        "\r\n",
        "    compute_accuracy_stream1(train_dataloader, valid_dataloader, encoder_stream1, decoder_stream1, n_epochs)\r\n",
        "    # showPlot(plot_losses)\r\n",
        "    save_model(encoder_stream1, decoder_stream1, encoder_stream2, decoder_stream2 , data, sufix, s2)\r\n",
        "    return encoder_stream1, decoder_stream1\r\n",
        "\r\n",
        "def eval(epochs, tr_seq_1, pred_seq_1, data, sufix, learning_rate=1e-3, loc=MODEL_LOC):\r\n",
        "    \r\n",
        "    encoder_stream1 = None\r\n",
        "    decoder_stream1 = None\r\n",
        "    encoder_stream2 = None\r\n",
        "    decoder_stream2 = None\r\n",
        "\r\n",
        "    encoder1loc = os.path.join(loc.format(data), 'encoder_stream1_{}{}.pt'.format(data, sufix))\r\n",
        "    decoder1loc = os.path.join(loc.format(data), 'decoder_stream1_{}{}.pt'.format(data, sufix))\r\n",
        "    encoder2loc = os.path.join(loc.format(data), 'encoder_stream2_{}{}.pt'.format(data, sufix))\r\n",
        "    decoder2loc = os.path.join(loc.format(data), 'decoder_stream2_{}{}.pt'.format(data, sufix))\r\n",
        "\r\n",
        "    train_raw = tr_seq_1\r\n",
        "    pred_raw = pred_seq_1\r\n",
        "#    train2_raw = tr_seq_2\r\n",
        "#    pred2_raw = pred_seq_2\r\n",
        "    # Initialize encoder, decoders for both streams\r\n",
        "    batch = load_batch ( 0 , BATCH_SIZE , 'pred' , train_raw , pred_raw , [], [], [], [] )\r\n",
        "    batch , _, _ = batch\r\n",
        "    batch_in_form = np.asarray ( [ batch[ i ][ 'sequence' ] for i in range ( BATCH_SIZE ) ] )\r\n",
        "    batch_in_form = torch.Tensor ( batch_in_form )\r\n",
        "    [ batch_size , step_size , fea_size ] = np.shape(batch_in_form)\r\n",
        "    input_dim = fea_size\r\n",
        "    hidden_dim = fea_size\r\n",
        "    output_dim = fea_size\r\n",
        "\r\n",
        "    encoder_stream1 = Encoder ( input_dim , hidden_dim , output_dim ).to ( device )\r\n",
        "    decoder_stream1 = Decoder ( 's1' , input_dim , hidden_dim , output_dim, batch_size, step_size ).to ( device )\r\n",
        "    encoder_stream1_optimizer = optim.RMSprop(encoder_stream1.parameters(), lr=learning_rate)\r\n",
        "    decoder_stream1_optimizer = optim.RMSprop(decoder_stream1.parameters(), lr=learning_rate)\r\n",
        "    encoder_stream1.load_state_dict(torch.load(encoder1loc))\r\n",
        "    encoder_stream1.eval()       \r\n",
        "    decoder_stream1.load_state_dict(torch.load(decoder1loc))        \r\n",
        "    decoder_stream1.eval()\r\n",
        "\r\n",
        "    \r\n",
        "    compute_accuracy_stream1(tr_seq_1, pred_seq_1, encoder_stream1, decoder_stream1, epochs) \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def train_stream1(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, stream2_output,batch_agent_ids, testbatch2, s2):\r\n",
        "\r\n",
        "    target_length = target_tensor.size(0)\r\n",
        "\r\n",
        "    Hidden_State , _ = encoder.loop(input_tensor)\r\n",
        "    _, _, mu_1, mu_2, log_sigma_1, log_sigma_2, rho = decoder.loop(Hidden_State)\r\n",
        "\r\n",
        "\r\n",
        "    encoder_optimizer.zero_grad()\r\n",
        "    decoder_optimizer.zero_grad()\r\n",
        "    if s2 == True:\r\n",
        "        cluster_centers = calculate_cluster_centers(batch_agent_ids, stream2_output, testbatch2)\r\n",
        "        cluster_centers = Variable(torch.Tensor(cluster_centers).to(device))\r\n",
        "        loss = -log_likelihood(mu_1, mu_2, log_sigma_1, log_sigma_2, rho, target_tensor, cluster_centers)\r\n",
        "    else:\r\n",
        "        loss = -log_likelihood(mu_1, mu_2, log_sigma_1, log_sigma_2, rho, target_tensor, None)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    loss = loss if loss >0 else -1*loss\r\n",
        "    loss.backward()\r\n",
        "\r\n",
        "    encoder_optimizer.step()\r\n",
        "    decoder_optimizer.step()\r\n",
        "    return loss.item() / target_length\r\n",
        "\r\n",
        "def save_model(encoder_stream1, decoder_stream1, encoder_stream2, decoder_stream2, data, sufix, s2, loc=MODEL_LOC):\r\n",
        "    torch.save(encoder_stream1.state_dict(), os.path.join(loc.format(data), 'encoder_stream1_{}{}.pt'.format(data, sufix)))\r\n",
        "    torch.save(decoder_stream1.state_dict(), os.path.join(loc.format(data), 'decoder_stream1_{}{}.pt'.format(data, sufix)))\r\n",
        "    if s2:\r\n",
        "        torch.save(encoder_stream2.state_dict(), os.path.join(loc.format(data), 'encoder_stream2_{}{}.pt'.format(data, sufix)))\r\n",
        "        torch.save(decoder_stream2.state_dict(), os.path.join(loc.format(data), 'decoder_stream2_{}{}.pt'.format(data, sufix)))\r\n",
        "    print('model saved at {}'.format(loc.format(data)))\r\n",
        "\r\n",
        "\r\n",
        "def train_stream2(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer):\r\n",
        "\r\n",
        "    Hidden_State , _ = encoder.loop(input_tensor)\r\n",
        "    stream2_out,_, _, _, _, _, _, _ = decoder.loop(Hidden_State)\r\n",
        "\r\n",
        "    l = nn.MSELoss()\r\n",
        "\r\n",
        "    encoder_optimizer.zero_grad()\r\n",
        "    decoder_optimizer.zero_grad()\r\n",
        "    loss = l(stream2_out, target_tensor)\r\n",
        "    loss.backward()\r\n",
        "\r\n",
        "    # loss = -log_likelihood(mu_1, mu_2, log_sigma_1, log_sigma_2, rho, target_tensor)\r\n",
        "    # print(loss)\r\n",
        "    encoder_optimizer.step()\r\n",
        "    decoder_optimizer.step()\r\n",
        "\r\n",
        "    return loss.item(), stream2_out\r\n",
        "\r\n",
        "def generate(inputs, encoder, decoder):\r\n",
        "    with torch.no_grad():\r\n",
        "        Hidden_State , Cell_State = encoder.loop(inputs)\r\n",
        "        decoder_hidden , decoder_cell , mu_1 , mu_2 , log_sigma_1 , log_sigma_2 , rho = decoder.loop(Hidden_State)\r\n",
        "        [ batch_size , step_size , fea_size ] = mu_1.size ()\r\n",
        "        out = []\r\n",
        "        for i in range(batch_size):\r\n",
        "            mu1_current = mu_1[ i , : , : ]\r\n",
        "            mu2_current = mu_2[ i , : , : ]\r\n",
        "            sigma1_current = log_sigma_1[ i , : , : ]\r\n",
        "            sigma2_current = log_sigma_2[ i , : , : ]\r\n",
        "            rho_current = rho[ i , : , : ]\r\n",
        "            out.append(sample(mu1_current , mu2_current , sigma1_current , sigma2_current , rho_current))\r\n",
        "\r\n",
        "        return np.array(out)\r\n",
        "\r\n",
        "\r\n",
        "def log_likelihood(mu_1, mu_2, log_sigma_1, log_sigma_2, rho, y, cluster_centers):\r\n",
        "    [batch_size, step_size, fea_size] = y.size()\r\n",
        "\r\n",
        "    epoch_loss = 0\r\n",
        "    for i in range(step_size):\r\n",
        "        mu1_current = mu_1[:,i,:]\r\n",
        "        mu2_current = mu_2[:,i,:]\r\n",
        "        if cluster_centers is not None:\r\n",
        "            muc_x = cluster_centers[:,i,0]\r\n",
        "            muc_y = cluster_centers[:,i,1]\r\n",
        "        sigma1_current = log_sigma_1[:,i,:]\r\n",
        "        sigma2_current = log_sigma_2[:,i,:]\r\n",
        "        rho_current = rho[:,i,:]\r\n",
        "        y_current = y[:,i,:]\r\n",
        "        if cluster_centers is None:\r\n",
        "            batch_loss = compute_sample_loss(mu1_current, mu2_current, sigma1_current, sigma2_current, rho_current, y_current).sum()\r\n",
        "        else:\r\n",
        "            batch_loss = compute_sample_loss(mu1_current, mu2_current, sigma1_current, sigma2_current, rho_current, y_current).sum() + torch.sqrt(torch.sum((mu1_current-muc_x)**2) + torch.sum((mu2_current-muc_y)**2))\r\n",
        "        batch_loss = batch_loss/batch_size\r\n",
        "        epoch_loss += batch_loss\r\n",
        "    return epoch_loss\r\n",
        "\r\n",
        "def compute_sample_loss(mu_1, mu_2,log_sigma_1, log_sigma_2, rho, y):\r\n",
        "    const = 1E-20 # to prevent numerical error\r\n",
        "    pi_term = torch.Tensor([2*np.pi]).to(device)\r\n",
        "\r\n",
        "    y_1 = y[:,0]\r\n",
        "    # y_1 = (y_1-torch.mean(y_1))/y_1.max()\r\n",
        "    y_2 = y[:,1]\r\n",
        "    # y_2 = (y_2 - torch.mean(y_2))/y_2.max()\r\n",
        "    mu_1 = torch.mean(y_1) + (y_1 -torch.mean(mu_1))\r\n",
        "    # mu_1 = torch.mean(y_1) + (y_1 -torch.mean(mu_1)) * (torch.std(y_1)/torch.std(mu_1))\r\n",
        "    mu_2 = torch.mean(y_2) + (y_2 -torch.mean(mu_2))\r\n",
        "    # mu_2 = torch.mean(y_2) + (y_2 -torch.mean(mu_2)) * ((torch.std(y_2))/(torch.std(mu_2)))\r\n",
        "    z = ( (y_1 - mu_1)**2/(log_sigma_1**2) + ((y_2 - mu_2)**2/(log_sigma_2**2)) - 2*rho*(y_1-mu_1)*(y_2-mu_2)/((log_sigma_1 *log_sigma_2)) )\r\n",
        "    mog_lik2 = torch.exp( (-1*z)/(2*(1-rho**2)) )\r\n",
        "    mog_lik1 =  1/(pi_term * log_sigma_1 * log_sigma_2 * (1-rho**2).sqrt() )\r\n",
        "    mog_lik = (mog_lik1*(mog_lik2+1e-8)).log()\r\n",
        "    return mog_lik\r\n",
        "\r\n",
        "def calculate_cluster_centers(agent_IDs, stream2_output, testbatch2):\r\n",
        "    stream2_output = stream2_output.cpu().detach().numpy()\r\n",
        "    num_batches = len(testbatch2)\r\n",
        "    num_steps = stream2_output.shape[1]\r\n",
        "    cluster_centers = np.zeros((num_batches, num_steps, 2))\r\n",
        "    km = KMeans(init='k-means++', n_clusters=3)\r\n",
        "    for t in range(num_steps):\r\n",
        "        for b in range(len(testbatch2) - 1):\r\n",
        "            clusters = km.fit(stream2_output[b, t, :].reshape(-1, 1))\r\n",
        "            cluster_indcs = np.where(clusters.labels_ == clusters.labels_[agent_IDs[b]])[0]\r\n",
        "            testbatch2_item = testbatch2[b]\r\n",
        "            keys = list(testbatch2_item.keys())\r\n",
        "            coords = testbatch2_item[keys[2 + t]][:, cluster_indcs]\r\n",
        "            cluster_centers[b, t, :] = np.mean(coords, axis=1)\r\n",
        "    return cluster_centers\r\n",
        "\r\n",
        "# ====================================== HELPER FUNCTIONS =========================================\r\n",
        "\r\n",
        "import time\r\n",
        "import math\r\n",
        "\r\n",
        "\r\n",
        "def asMinutes(s):\r\n",
        "    m = math.floor(s / 60)\r\n",
        "    s -= m * 60\r\n",
        "    return '%dm %ds' % (m, s)\r\n",
        "\r\n",
        "\r\n",
        "def timeSince(since, percent):\r\n",
        "    now = time.time()\r\n",
        "    s = now - since\r\n",
        "    es = s / (percent)\r\n",
        "    rs = es - s\r\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\r\n",
        "\r\n",
        "def sample(mu_1 , mu_2 , log_sigma_1 , log_sigma_2, rho):\r\n",
        "\r\n",
        "    sample = []\r\n",
        "    for i in range(len(mu_1)):\r\n",
        "        mu = np.array([mu_1[i][0].item(), mu_2[i][0].item()])\r\n",
        "        sigma_1 = log_sigma_1[i][0].item()\r\n",
        "        sigma_2 = log_sigma_2[i][0].item()\r\n",
        "        c = rho[i][0].item() * sigma_1 * sigma_2\r\n",
        "        cov = np.array([[sigma_1**2, c],[c, sigma_2**2]])\r\n",
        "        sample.append(np.random.multivariate_normal(mu, cov))\r\n",
        "\r\n",
        "    return sample\r\n",
        "\r\n",
        "def computeDist ( x1 , y1, x2, y2 ):\r\n",
        "    return np.sqrt( pow ( x1 - x2 , 2 ) + pow ( y1 - y2 , 2 ) )\r\n",
        "\r\n",
        "def computeKNN ( curr_dict , ID , k ):\r\n",
        "    import heapq\r\n",
        "    from operator import itemgetter\r\n",
        "\r\n",
        "    ID_x = curr_dict[ ID ][0]\r\n",
        "    ID_y = curr_dict[ ID ][1]\r\n",
        "    dists = {}\r\n",
        "    for j in range ( len ( curr_dict ) ):\r\n",
        "        if j != ID:\r\n",
        "            dists[ j ] = computeDist ( ID_x , ID_y, curr_dict[ j ][0],curr_dict[ j ][1] )\r\n",
        "    KNN_IDs = dict ( heapq.nsmallest ( k , dists.items () , key=itemgetter ( 1 ) ) )\r\n",
        "    neighbors = list ( KNN_IDs.keys () )\r\n",
        "\r\n",
        "    return neighbors\r\n",
        "\r\n",
        "def compute_A ( frame ):\r\n",
        "    A = np.zeros ( [ frame.shape[ 0 ] , frame.shape[ 0 ] ] )\r\n",
        "    for i in range ( len ( frame ) ):\r\n",
        "        if frame[ i ] is not None:\r\n",
        "            neighbors = computeKNN ( frame , i , 4 )\r\n",
        "        for neighbor in neighbors:\r\n",
        "            A[ i ][ neighbor ] = 1\r\n",
        "    return A\r\n",
        "\r\n",
        "\r\n",
        "def compute_accuracy_stream1(traindataloader, labeldataloader, encoder, decoder, n_epochs):\r\n",
        "    ade = 0\r\n",
        "    fde = 0\r\n",
        "    count = 0\r\n",
        "\r\n",
        "\r\n",
        "    train_raw = traindataloader\r\n",
        "    pred_raw = labeldataloader\r\n",
        "    train2_raw = []\r\n",
        "    pred2_raw = []\r\n",
        "\r\n",
        "    batch = load_batch(0, BATCH_SIZE, 'pred', train_raw, pred_raw, train2_raw, pred2_raw, [], [])\r\n",
        "    batch, _, _ = batch\r\n",
        "    batch_in_form = np.asarray([batch[i]['sequence'] for i in range(BATCH_SIZE)])\r\n",
        "    batch_in_form = torch.Tensor(batch_in_form)\r\n",
        "    [ batch_size , step_size , fea_size ] = np.shape(batch_in_form)\r\n",
        "\r\n",
        "    print('computing accuracy...')\r\n",
        "    for epoch in range(0, n_epochs):\r\n",
        "        # Prepare train and test batch\r\n",
        "        if epoch % (int(n_epochs/10) + 1) == 0:\r\n",
        "            print(\"{}/{} in computing accuracy...\".format(epoch, n_epochs))\r\n",
        "        trainbatch_both = load_batch ( epoch , BATCH_SIZE , 'train' , train_raw , pred_raw, train2_raw, pred2_raw, [], [] )\r\n",
        "        trainbatch, trainbatch2, _ = trainbatch_both\r\n",
        "        trainbatch_in_form = np.asarray([trainbatch[i]['sequence'] for i in range(len(trainbatch))])\r\n",
        "        trainbatch_in_form = torch.Tensor ( trainbatch_in_form )\r\n",
        "\r\n",
        "        testbatch_both = load_batch ( epoch , BATCH_SIZE , 'pred' , train_raw , pred_raw, train2_raw, pred2_raw, [], [] )\r\n",
        "        testbatch, testbatch2, _  = testbatch_both\r\n",
        "        testbatch_in_form = np.asarray([testbatch[i]['sequence'] for i in range(len(trainbatch))])\r\n",
        "        testbatch_in_form = torch.Tensor ( testbatch_in_form )\r\n",
        "\r\n",
        "        train = trainbatch_in_form.to(device)\r\n",
        "        label = testbatch_in_form.to(device)\r\n",
        "\r\n",
        "        pred = generate(train, encoder, decoder)\r\n",
        "        mse = MSE(pred, label)\r\n",
        "        # print(mse)\r\n",
        "        mse = np.sqrt(mse)\r\n",
        "        ade += mse\r\n",
        "        fde += mse[-1]\r\n",
        "        # count += testbatch_in_form.size()[0]\r\n",
        "        count +=1  \r\n",
        "    \r\n",
        "    ade = ade/count\r\n",
        "    fde = fde/count\r\n",
        "    print('RMSE: {}'.format(ade))\r\n",
        "    print(\"ADE: {} FDE: {}\".format(np.mean(ade), fde))\r\n",
        "\r\n",
        "def makeplot(x, y, x_label, y_label, title, save_loc):\r\n",
        "    fig = plt.figure(title)\r\n",
        "    plt.plot(x, y)\r\n",
        "    plt.xlabel(x_label)\r\n",
        "    plt.ylabel(y_label)\r\n",
        "    plt.title(title)\r\n",
        "    fig.savefig(os.path.join(save_loc, title+'.png'))\r\n",
        "    fig.clear()\r\n",
        "\r\n",
        "def MSE(y_pred, y_gt, device=device):\r\n",
        "    # y_pred = y_pred.numpy()\r\n",
        "    y_gt = y_gt.cpu().detach().numpy()\r\n",
        "    acc = np.zeros(np.shape(y_pred)[:-1])\r\n",
        "    muX = y_pred[:,:,0]\r\n",
        "    muY = y_pred[:,:,1]\r\n",
        "    x = np.array(y_gt[:,:, 0])\r\n",
        "    x = (x-np.mean(x))/x.std()\r\n",
        "    y = np.array(y_gt[:,:, 1])\r\n",
        "    # muX = np.mean(x) + (x - np.mean(muX)) * (np.std(x))/(np.std(muX))\r\n",
        "    # muX = np.mean(x) + (x - np.mean(muX))\r\n",
        "    y = (y-np.mean(y))/y.std()\r\n",
        "    # muY = np.mean(y) + (y -np.mean(muY)) * (np.std(y))/(np.std(muY))\r\n",
        "    # muY = np.mean(y) + (y -np.mean(muY))\r\n",
        "    acc = np.power(x-muX, 2) + np.power(y-muY, 2)\r\n",
        "    lossVal = np.sum(acc, axis=0)/len(acc)\r\n",
        "    return lossVal\r\n",
        "\r\n",
        "def compute_eigs ( train_stream2):\r\n",
        "    N = train_stream2[0][list ( train_stream2[ 0 ].keys())[ -1]].shape[ 1 ]\r\n",
        "    A = np.zeros ( [ N, N ] )\r\n",
        "    frame = {}\r\n",
        "    eig_batch = []\r\n",
        "    for batch_idx in range(len(train_stream2)):\r\n",
        "        eig_frame = []\r\n",
        "        for which_frame in list ( train_stream2[ batch_idx ].keys())[ 2: ]:\r\n",
        "            for j in range(N):\r\n",
        "                frame[j] = [train_stream2[batch_idx][which_frame][0,j],train_stream2[batch_idx][which_frame][1,j]]\r\n",
        "            for l in range (N):\r\n",
        "                if frame[ l ] is not None:\r\n",
        "                    neighbors = computeKNN ( frame , l , 4 )\r\n",
        "                for neighbor in neighbors:\r\n",
        "                    # if neighbor in labels:\r\n",
        "                    # if idx < labels.index ( neighbor ):\r\n",
        "                    dist_of_neighbor = computeDist(frame[l][0],frame[l][1], frame[neighbor][0],frame[neighbor][1])\r\n",
        "                    if dist_of_neighbor <= MU:\r\n",
        "                        A[ l ][ neighbor ] = np.exp(-1*computeDist(frame[l][0],frame[l][1], frame[neighbor][0],frame[neighbor][1]))\r\n",
        "            d = [ np.sum ( A[ row , : ] ) for row in range ( A.shape[ 0 ] ) ]\r\n",
        "            D = np.diag ( d )\r\n",
        "            L = D - A\r\n",
        "            _, vecs = eigs(L, k=2 )\r\n",
        "            eig_frame.append(np.real(vecs[:,1]))\r\n",
        "        eig_batch.append ( np.array(eig_frame) )\r\n",
        "    return torch.Tensor(np.array(eig_batch))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-ae2cfc3933bd>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    from /content/drive/MyDrive/Colab Notebooks/models.ipynb import *\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}